# -*- coding: utf-8 -*-
"""DataThon_(1) (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zYObY4J8hV7ViK4nWRnUo4MLPW2b5w-Y
"""

import pandas as pd

df = pd.read_csv("Train.csv")
df.head()

df.isnull()

df.isnull().sum()

len(df)

numerical_columns = [ 'SeedlingsPerPit', 'TransplantingIrrigationHours', 'Ganaura', 'CropOrgFYM',
                     'BasalDAP', 'BasalUrea', '1tdUrea', '1appDaysUrea', '2tdUrea', '2appDaysUrea']

for col in numerical_columns:
    df[col].fillna(df[col].mean(), inplace=True)

df.isnull().sum()

categorical_columns = ['TransplantingIrrigationSource', 'TransplantingIrrigationPowerSource', 'OrgFertilizers', 'PCropSolidOrgFertAppMethod', 'MineralFertAppMethod.1']

for col in categorical_columns:
    df[col].fillna(df[col].mode()[0], inplace=True)

# You can choose to drop rows with missing dates
df.dropna(subset=['RcNursEstDate'], inplace=True)

# OR

# Fill missing dates with a default date (e.g., '1900-01-01')
df['RcNursEstDate'].fillna('1900-01-01', inplace=True)

# Fill missing values in the 'TransIrriCost' column with a specific value (e.g., 0)
df['TransIrriCost'].fillna(0, inplace=True)

df.isnull().sum()

len(df)

df.head()

df.shape[1]

# Fill missing values in the 'TransIrriCost' column with a specific value (e.g., 0)
df['TransIrriCost'].fillna(0, inplace=True)

# Fill missing values in the 'NursDetFactor' column with a specific value (e.g., 'Unknown')
df['NursDetFactor'].fillna('Unknown', inplace=True)

# Fill missing values in the 'StandingWater' column with a specific value (e.g., -1 for indicating missing)
df['StandingWater'].fillna(-1, inplace=True)

# Fill missing values in the 'CropbasalFerts' column with a specific value (e.g., 'NotAvailable')
df['CropbasalFerts'].fillna('NotAvailable', inplace=True)

# Fill missing values in the 'FirstTopDressFert' column with a specific value (e.g., 'NotApplicable')
df['FirstTopDressFert'].fillna('NotApplicable', inplace=True)

# Fill missing values in the 'Harv_hand_rent' column with a specific value (e.g., 0)
df['Harv_hand_rent'].fillna(0, inplace=True)

df.isnull().sum()

len(df)

# Fill missing values in the 'TransDetFactor' column with a specific value (e.g., 'Missing')
df['TransDetFactor'].fillna('Missing', inplace=True)

df.isnull().sum()

len(df)

df.head()

df1 = pd.read_csv("Test.csv")

df1.isnull().sum()

len(df1)

numerical_columns = [ 'SeedlingsPerPit', 'TransplantingIrrigationHours', 'Ganaura', 'CropOrgFYM',
                     'BasalDAP', 'BasalUrea', '1tdUrea', '1appDaysUrea', '2tdUrea', '2appDaysUrea']

for col in numerical_columns:
    df1[col].fillna(df1[col].mean(), inplace=True)

categorical_columns = ['TransplantingIrrigationSource', 'TransplantingIrrigationPowerSource', 'OrgFertilizers', 'PCropSolidOrgFertAppMethod', 'MineralFertAppMethod.1']

for col in categorical_columns:
    df1[col].fillna(df1[col].mode()[0], inplace=True)

# You can choose to drop rows with missing dates
df1.dropna(subset=['RcNursEstDate'], inplace=True)

# OR

# Fill missing dates with a default date (e.g., '1900-01-01')
df1['RcNursEstDate'].fillna('1900-01-01', inplace=True)

# Fill missing values in the 'TransIrriCost' column with a specific value (e.g., 0)
df1['TransIrriCost'].fillna(0, inplace=True)

# Fill missing values in the 'TransIrriCost' column with a specific value (e.g., 0)
df1['TransIrriCost'].fillna(0, inplace=True)

# Fill missing values in the 'NursDetFactor' column with a specific value (e.g., 'Unknown')
df1['NursDetFactor'].fillna('Unknown', inplace=True)

# Fill missing values in the 'StandingWater' column with a specific value (e.g., -1 for indicating missing)
df1['StandingWater'].fillna(-1, inplace=True)

# Fill missing values in the 'CropbasalFerts' column with a specific value (e.g., 'NotAvailable')
df1['CropbasalFerts'].fillna('NotAvailable', inplace=True)

# Fill missing values in the 'FirstTopDressFert' column with a specific value (e.g., 'NotApplicable')
df1['FirstTopDressFert'].fillna('NotApplicable', inplace=True)

# Fill missing values in the 'Harv_hand_rent' column with a specific value (e.g., 0)
df1['Harv_hand_rent'].fillna(0, inplace=True)

# Fill missing values in the 'TransDetFactor' column with a specific value (e.g., 'Missing')
df1['TransDetFactor'].fillna('Missing', inplace=True)

df1.isnull().sum()

len(df1)

df.head()

import pandas as pd

# Load your dataset into a Pandas DataFrame


# Encode categorical columns into numerical values

# 1. Label Encoding for Ordinal Categorical Data
from sklearn.preprocessing import LabelEncoder

ordinal_categorical_columns = ['LandPreparationMethod', 'CropEstMethod', 'NursDetFactor', 'TransDetFactor',
                               'TransplantingIrrigationSource', 'TransplantingIrrigationPowerSource',
                               'CropbasalFerts', 'MineralFertAppMethod', 'MineralFertAppMethod',
                               'Harv_method', 'Threshing_method', 'Stubble_use']

le = LabelEncoder()
for column in ordinal_categorical_columns:
    df[column] = le.fit_transform(df[column])

# 2. One-Hot Encoding for Nominal Categorical Data
nominal_categorical_columns = ['District', 'Block', 'OrgFertilizers']

df = pd.get_dummies(df, columns=nominal_categorical_columns)

# You can also handle other categorical columns depending on their nature.

# 3. Custom Mapping for Binary Categorical Data
# For binary categorical columns, you can manually map values to 0 and 1.
'''binary_categorical_columns = ['TransplantingIrrigationHours']

binary_mapping = {'No': 0, 'Yes': 1}
for column in binary_categorical_columns:
    df[column] = df[column].map(binary_mapping)

# Ensure that all columns are numeric
df = df.apply(pd.to_numeric, errors='ignore')

# Now, your dataset has numerical representations of categorical data.

# You can use df.head() to check the updated DataFrame.'''

df.head(10)

df.info()

import pandas as pd

# Load your dataset into a Pandas DataFrame


# Encode categorical columns into numerical values

# 1. Label Encoding for Ordinal Categorical Data
from sklearn.preprocessing import LabelEncoder

ordinal_categorical_columns = ['LandPreparationMethod', 'CropEstMethod', 'NursDetFactor', 'TransDetFactor',
                               'TransplantingIrrigationSource', 'TransplantingIrrigationPowerSource',
                               'CropbasalFerts', 'MineralFertAppMethod','PCropSolidOrgFertAppMethod','FirstTopDressFert',
                               'Harv_method', 'Threshing_method', 'Stubble_use']

le = LabelEncoder()
for column in ordinal_categorical_columns:
    df1[column] = le.fit_transform(df1[column])

# 2. One-Hot Encoding for Nominal Categorical Data
nominal_categorical_columns = ['District', 'Block', 'OrgFertilizers']

df1 = pd.get_dummies(df1, columns=nominal_categorical_columns)

# You can also handle other categorical columns depending on their nature.

# 3. Custom Mapping for Binary Categorical Data
# For binary categorical columns, you can manually map values to 0 and 1.
binary_categorical_columns = ['TransplantingIrrigationHours']

'''binary_mapping = {'No': 0, 'Yes': 1}
for column in binary_categorical_columns:
    df1[column] = df1[column].map(binary_mapping)

# Ensure that all columns are numeric
df1 = df1.apply(pd.to_numeric, errors='ignore')

# Now, your dataset has numerical representations of categorical data.

# You can use df.head() to check the updated DataFrame.'''

df1.head()

print(df.columns)

df.drop(['MineralFertAppMethod.1'], axis=1, inplace=True)

df1.drop(['MineralFertAppMethod.1'], axis=1, inplace=True)

df.info()

df.head()

import pandas as pd

# Replace 'PlaceholderDateColumn' with the actual column names in your DataFrame
date_columns = ['CropTillageDate', 'RcNursEstDate', 'Harv_date', 'Threshing_date', 'SeedingSowingTransplanting']

for column in date_columns:
    df[column] = pd.to_datetime(df[column], errors='coerce')

# Extract year, month, and day components
df['CropTillageYear'] = df['CropTillageDate'].dt.year
df['CropTillageMonth'] = df['CropTillageDate'].dt.month
df['CropTillageDay'] = df['CropTillageDate'].dt.day

df['RcNursEstYear'] = df['RcNursEstDate'].dt.year
df['RcNursEstMonth'] = df['RcNursEstDate'].dt.month
df['RcNursEstDay'] = df['RcNursEstDate'].dt.day

df['HarvYear'] = df['Harv_date'].dt.year
df['HarvMonth'] = df['Harv_date'].dt.month
df['HarvDay'] = df['Harv_date'].dt.day

df['ThreshingYear'] = df['Threshing_date'].dt.year
df['ThreshingMonth'] = df['Threshing_date'].dt.month
df['ThreshingDay'] = df['Threshing_date'].dt.day

df['SeedingSowingTransplantingYear'] = df['SeedingSowingTransplanting'].dt.year
df['SeedingSowingTransplantingMonth'] = df['SeedingSowingTransplanting'].dt.month
df['SeedingSowingTransplantingDay'] = df['SeedingSowingTransplanting'].dt.day

df.head()

df = df.drop(['CropTillageDate', 'RcNursEstDate', 'Harv_date', 'Threshing_date','SeedingSowingTransplanting'], axis=1)

df.info()

import pandas as pd

# Replace 'PlaceholderDateColumn' with the actual column names in your DataFrame
date_columns = ['CropTillageDate', 'RcNursEstDate', 'Harv_date', 'Threshing_date', 'SeedingSowingTransplanting']

for column in date_columns:
    df1[column] = pd.to_datetime(df1[column], errors='coerce')

# Extract year, month, and day components
df1['CropTillageYear'] = df1['CropTillageDate'].dt.year
df1['CropTillageMonth'] = df1['CropTillageDate'].dt.month
df1['CropTillageDay'] = df1['CropTillageDate'].dt.day

df1['RcNursEstYear'] = df1['RcNursEstDate'].dt.year
df1['RcNursEstMonth'] = df1['RcNursEstDate'].dt.month
df1['RcNursEstDay'] = df1['RcNursEstDate'].dt.day

df1['HarvYear'] = df1['Harv_date'].dt.year
df1['HarvMonth'] = df1['Harv_date'].dt.month
df1['HarvDay'] = df1['Harv_date'].dt.day

df1['ThreshingYear'] = df1['Threshing_date'].dt.year
df1['ThreshingMonth'] = df1['Threshing_date'].dt.month
df1['ThreshingDay'] = df1['Threshing_date'].dt.day

df1['SeedingSowingTransplantingYear'] = df1['SeedingSowingTransplanting'].dt.year
df1['SeedingSowingTransplantingMonth'] = df1['SeedingSowingTransplanting'].dt.month
df1['SeedingSowingTransplantingDay'] = df1['SeedingSowingTransplanting'].dt.day



df1 = df1.drop(['CropTillageDate', 'RcNursEstDate', 'Harv_date', 'Threshing_date','SeedingSowingTransplanting'], axis=1)

df1.info()

df.head()

df1.head()

df.drop(['PCropSolidOrgFertAppMethod', 'FirstTopDressFert'],axis=1,inplace = True)

non_numeric_columns = df.select_dtypes(exclude=['number']).columns
print(non_numeric_columns)

non_numeric_columns = df1.select_dtypes(exclude=['number']).columns
print(non_numeric_columns)

df.info()

df.info()

df.head()

df.drop(['TransplantingIrrigationHours'],axis =1,inplace = True)

# Drop inconsistent columns from df1
columns_to_drop = [
    'OrgFertilizers_FYM Ganaura VermiCompost',
    'OrgFertilizers_FYM Ghanajeevamrit',
    # ... (list other inconsistent columns here)
]
df = df.drop(columns=columns_to_drop)

df.info()

df1.info()

common_columns_list = common_columns.tolist()
common_columns_list

df1['Yield'] = df['Yield']

# Identify common columns
common_columns = df.columns.intersection(df1.columns)

# Reorder columns in df to match df1
df = df[common_columns]

# Reorder columns in df1 to match df
df1 = df1[common_columns]

# Now df and df1 have the same columns, including 'Yield' in both

df1.drop(['Yield'],axis =1,inplace = True)

df1.info()

df.info()

import pandas as pd
from sklearn.decomposition import PCA
import numpy as np

# Load your training dataset
# Replace 'df' with the actual name of your training DataFrame

# Define the target variable
y = df['Yield']

# Drop non-numeric columns and unnecessary columns
columns_to_drop = ['ID']  # Add other non-numeric columns here if necessary
X = df.drop(columns=columns_to_drop)

# Perform PCA with the desired number of components (e.g., 20)
n_components = 5
pca = PCA(n_components=n_components)
X_pca = pca.fit_transform(X)

# Get the principal components (features) that contribute to each PCA component
components = pca.components_

# Create a DataFrame to display the feature names for each PCA component
feature_names = X.columns
component_names = [f'Component {i+1}' for i in range(n_components)]
components_df = pd.DataFrame(components, columns=feature_names, index=component_names)

# Display the top 20 important feature names for the first PCA component (you can adjust this)
top_20_features = components_df.iloc[0].abs().sort_values(ascending=False).head(5)
print(top_20_features)

# Assuming you have the 'ID' column separately, replace 'id_column' with the actual 'ID' column data
id_column =df['ID']

import pandas as pd

# Load your original dataset
# Replace 'df' with the actual name of your DataFrame

# Define the feature names to keep based on importance scores
important_features = [
    'Yield', 'TransIrriCost', 'BasalDAP', 'CultLand',
    'CropCultLand']


# Create a new DataFrame with only the important columns
df= df[important_features]


# Assuming you have a separate 'ID' column, add it back to the DataFrame
df['ID'] = id_column

df1['ID'] = id_column

# List of important feature names
import pandas as pd

# Load your original dataset
# Replace 'df' with the actual name of your DataFrame

# Define the feature names to keep based on importance scores
important_features1 = [
      'TransIrriCost', 'BasalDAP', 'CultLand',
    'CropCultLand'
]

# Create a new DataFrame with only the important columns
df1 = df[important_features1]


# Assuming you have a separate 'ID' column, add it back to the DataFrame
df1 = df1[important_features1]
df1['ID'] = id_col1

df.info()

df1.info()

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# Handle date columns (if not already done)

# Drop non-numeric columns and unnecessary columns
columns_to_drop = ['ID', 'Yield']  # Add other non-numeric columns here if necessary

# Load your training dataset
# Assuming you have a DataFrame df containing your training data
# Replace 'df' with the actual name of your training DataFrame

# Initialize a Random Forest regressor (you can use another regression model)
rf_model = RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42)

# Split the data into training and validation sets (optional but recommended)
X = df.drop(columns=columns_to_drop)
y = df['Yield']
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model on the training dataset
rf_model.fit(X_train, y_train)

# Preprocess your test dataset (df1) as you did for the training dataset
# Ensure that the columns match the training dataset

# Select columns from the test dataset in the same order as the training dataset
X_test = df1[X_train.columns]

# Use the trained model to make predictions on the testing dataset
test_predictions = rf_model.predict(X_test)

# Create a submission DataFrame
submission_df = pd.DataFrame({'ID': df1['ID'], 'Yield': test_predictions})

# Save the submission DataFrame to a CSV file
submission_df.to_csv('submission_rf.csv', index=False)





from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Assuming you have the actual target values (y_true) and the predicted values (y_pred)

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_val, y_pred)

# Calculate Root Mean Squared Error (RMSE)
rmse = mean_squared_error(y_val, y_pred, squared=False)

# Calculate R-squared (R²) score
r2 = r2_score(y_val, y_pred)

# Print the results
print(f'Mean Absolute Error (MAE): {mae:.4f}')
print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')
print(f'R-squared (R²) Score: {r2:.4f}')

from sklearn.metrics import mean_absolute_error

# Assuming you have actual target values y_val and predicted values y_pred
mae = mean_absolute_error(y_val, y_pred)

# Print the Mean Absolute Error
print(f'Mean Absolute Error (MAE): {mae:.4f}')

import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np

# Load your training dataset
# Assuming you have a DataFrame df containing your training data
# Replace 'df' with the actual name of your training DataFrame

# Define the target variable
y = df['Yield']

# Drop non-numeric columns and unnecessary columns
columns_to_drop = ['ID', 'Yield']  # Add other non-numeric columns here if necessary
X = df.drop(columns=columns_to_drop)

# Split the data into training and validation sets (optional but recommended)
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize a Random Forest regressor with hyperparameter tuning
rf_model = RandomForestRegressor(
    n_estimators=200,  # Increase the number of trees
    max_depth=10,  # Increase the depth of the trees
    min_samples_split=5,  # Adjust min_samples_split as needed
    min_samples_leaf=2,  # Adjust min_samples_leaf as needed
    random_state=42
)

# Train the model on the training dataset
rf_model.fit(X_train, y_train)

# Make predictions on the validation set
y_pred = rf_model.predict(X_val)

# Calculate RMSE (Root Mean Squared Error)
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
print(f'Validation RMSE: {rmse:.4f}')

# Preprocess your test dataset (df1) as you did for the training dataset
# Ensure that the columns match the training dataset

# Select columns from the test dataset in the same order as the training dataset
X_test = df1[X_train.columns]

# Use the trained model to make predictions on the testing dataset
test_predictions = rf_model.predict(X_test)

# Create a submission DataFrame
submission_df = pd.DataFrame({'ID': df1['ID'], 'Yield': test_predictions})

# Save the submission DataFrame to a CSV file
submission_df.to_csv('submission_rf.csv', index=False)

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load your training dataset (assuming you have a DataFrame called 'df')
# Replace 'df' with the actual name of your training DataFrame

# Drop non-numeric columns and unnecessary columns
columns_to_drop = ['ID', 'Yield']  # Add other non-numeric columns here if necessary
X = df.drop(columns=columns_to_drop)

# Calculate the correlation matrix
correlation_matrix = X.corr()

# Create a heatmap for visualization
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)
plt.title("Correlation Matrix")
plt.show()

import pandas as pd
from scipy import stats

# Load your training dataset (assuming you have a DataFrame called 'df')
# Replace 'df' with the actual name of your training DataFrame

# Drop non-numeric columns and unnecessary columns
columns_to_drop = ['ID', 'Yield']  # Add other non-numeric columns here if necessary
X = df.drop(columns=columns_to_drop)

# Calculate the Z-scores for each feature
z_scores = stats.zscore(X)

# Define a threshold for identifying outliers (e.g., Z-score > 3 or < -3)
threshold = 3

# Find the indices of outlier rows
outlier_indices = (z_scores > threshold).any(axis=1)

# Get the rows containing outliers
outliers = df[outlier_indices]

# Display or save the rows with outliers
print("Outliers:")
print(outliers)

# If you want to remove the rows with outliers, you can do this:
# df_no_outliers = df[~outlier_indices]

import matplotlib.pyplot as plt
import seaborn as sns

# Load your training dataset (assuming you have a DataFrame called 'df')
# Replace 'df' with the actual name of your training DataFrame

# Drop non-numeric columns and unnecessary columns
columns_to_drop = ['ID', 'Yield']  # Add other non-numeric columns here if necessary
X = df.drop(columns=columns_to_drop)

# Create a box plot for each numeric feature
plt.figure(figsize=(12, 6))
sns.boxplot(data=X)
plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility
plt.title('Box Plot for Numeric Features')
plt.xlabel('Features')
plt.ylabel('Values')
plt.show()

columns_with_outliers = ['SeedlingsPerPit', 'Residue_length']

# Function to remove outliers using IQR method
def remove_outliers_iqr(data, column):
    Q1 = data[column].quantile(0.25)
    Q3 = data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]

# Remove outliers for specified columns
for column in columns_with_outliers:
    df = remove_outliers_iqr(df, column)

import matplotlib.pyplot as plt
import seaborn as sns

# Load your training dataset (assuming you have a DataFrame called 'df')
# Replace 'df' with the actual name of your training DataFrame

# Drop non-numeric columns and unnecessary columns
columns_to_drop = ['ID', 'Yield']  # Add other non-numeric columns here if necessary
X = df.drop(columns=columns_to_drop)

# Create a box plot for each numeric feature
plt.figure(figsize=(12, 6))
sns.boxplot(data=X)
plt.xticks(rotation=45)  # Rotate x-axis labels for better visibility
plt.title('Box Plot for Numeric Features')
plt.xlabel('Features')
plt.ylabel('Values')
plt.show()

import pickle

# Assuming you have trained your model and stored it in a variable (replace 'your_model' with your model variable name)
your_model = 'rf_model'

# Specify the file path where you want to save the model
model_filename = "crop_yield_rf_model.pkl"

# Open the file in binary write mode
with open(model_filename, 'wb') as model_file:
    # Serialize and save the model to the file
    pickle.dump(your_model, model_file)

print(f"Model saved as {model_filename}")

import pickle

# Load the pickled model
with open('crop_yield_rf_model.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

# Now, 'loaded_model' is your trained Random Forest model, and you can use it for predictions